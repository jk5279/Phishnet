{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5d6781973b313e1",
   "metadata": {},
   "source": [
    "# DL Pipeline for NLP Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424a35ca25695a0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T19:06:49.947044500Z",
     "start_time": "2025-10-30T19:06:17.315534Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jk5279/miniconda3/envs/WSLscam/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Deep Learning (Transformer) Preprocessing Pipeline (B1-B3)...\n",
      "--- [Step B1] Running DL (Transformer) Text Cleaning ---\n",
      "Loading: cleaned_data/master_email_dataset.csv\n",
      "Cleaning text using 16 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DL Cleaning: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 249873/249873 [00:03<00:00, 75941.21it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Dropped 161 rows that became empty after cleaning.\n",
      " - Saved DL cleaned: cleaned_data/dl_dataset_cleaned.csv (249712 rows)\n",
      "--- [Step B1] Complete ---\n",
      "\n",
      "--- [Step B2] Running Deduplication ---\n",
      "\n",
      "==================================\n",
      "Deduplication Report\n",
      "  Rows before: 249712\n",
      "  Rows after:  221899\n",
      "  Removed:     27813\n",
      "==================================\n",
      " - Saved DL deduplicated: cleaned_data/dl_dataset_deduped.csv (221899 unique rows)\n",
      "--- [Step B2] Complete ---\n",
      "\n",
      "--- [Step B3] Running Length Filtering ---\n",
      "Filtering rows with token count < 5 or > 2000...\n",
      "\n",
      "==================================\n",
      "Length Filtering Report\n",
      "  Rows before: 221899\n",
      "  Rows after:  216259\n",
      "  Removed:     5640\n",
      "==================================\n"
     ]
    }
   ],
   "source": [
    "# python\n",
    "import os\n",
    "import re\n",
    "import quopri\n",
    "import multiprocessing\n",
    "import warnings\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup, MarkupResemblesLocatorWarning\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "\n",
    "# Suppress the BeautifulSoup URL warning\n",
    "warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
    "\n",
    "# =========================\n",
    "# Configuration\n",
    "# =========================\n",
    "\n",
    "# --- Step B1 Configuration ---\n",
    "# Use the master file from your *first* pipeline as the input\n",
    "DL_INPUT_FILE = \"cleaned_data/master_email_dataset.csv\"\n",
    "DL_CLEANED_OUTPUT_FILE = \"cleaned_data/dl_dataset_cleaned.csv\"\n",
    "\n",
    "# --- Step B2 Configuration ---\n",
    "DL_DEDUPED_OUTPUT_FILE = \"cleaned_data/dl_dataset_deduped.csv\"\n",
    "\n",
    "# --- Step B3 Configuration ---\n",
    "# Filter settings: remove texts with < 5 tokens or > 2000 tokens.\n",
    "# We use 2000 as a loose upper bound. The tokenizer will handle\n",
    "# the final truncation to 512 tokens.\n",
    "MIN_TOKEN_LENGTH = 5\n",
    "MAX_TOKEN_LENGTH = 2000\n",
    "DL_FINAL_OUTPUT_FILE = \"cleaned_data/dl_dataset_final.csv\"\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# Step B1: Text Cleaning (Transformer Path)\n",
    "# ===================================================================\n",
    "\n",
    "def clean_email_text_dl(text: object) -> str:\n",
    "    \"\"\"\n",
    "    Gentle cleaning function for the Transformer (DL) pipeline.\n",
    "\n",
    "    CRITICAL: Does NOT lowercase or remove punctuation.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # 1. Fix Encoding Artifacts (Quoted-Printable)\n",
    "    try:\n",
    "        text_bytes = text.encode(\"latin-1\", errors=\"ignore\")\n",
    "        decoded_bytes = quopri.decodestring(text_bytes)\n",
    "        text = decoded_bytes.decode(\"utf-8\", errors=\"ignore\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 2. Strip HTML Tags\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    text = soup.get_text(separator=\" \")\n",
    "\n",
    "    # 3. Replace URLs with [URL]\n",
    "    # We add spaces around tokens to ensure they are tokenized correctly\n",
    "    text = re.sub(r\"(https?://\\S+|www\\.\\S+)\", \" [URL] \", text)\n",
    "\n",
    "    # 4. Replace Emails with [EMAIL]\n",
    "    text = re.sub(r\"\\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\\b\", \" [EMAIL] \", text)\n",
    "\n",
    "    # 5. Normalize Whitespace (replace \\n, \\t, etc. with a single space)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # --- NO lowercasing, NO punctuation removal ---\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_dataset_dl(\n",
    "    input_filename: str,\n",
    "    output_filename: str,\n",
    "    workers: int,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads the master dataset and applies the gentle DL cleaning.\n",
    "    \"\"\"\n",
    "    print(f\"--- [Step B1] Running DL (Transformer) Text Cleaning ---\")\n",
    "    print(f\"Loading: {input_filename}\")\n",
    "    try:\n",
    "        df = pd.read_csv(input_filename)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {input_filename}. Make sure it exists. Error: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = df.dropna(subset=[\"text\"])\n",
    "    df = df.copy()\n",
    "\n",
    "    print(f\"Cleaning text using {workers} workers...\")\n",
    "    df[\"text\"] = df[\"text\"].fillna(\"\")\n",
    "    results = process_map(\n",
    "        clean_email_text_dl,\n",
    "        df[\"text\"],\n",
    "        max_workers=workers,\n",
    "        chunksize=500,\n",
    "        desc=\"DL Cleaning\",\n",
    "    )\n",
    "    df[\"text\"] = results\n",
    "\n",
    "    # Text Integrity Validation\n",
    "    before = len(df)\n",
    "    df = df[df[\"text\"].str.strip().str.len() > 0]\n",
    "    dropped = before - len(df)\n",
    "    if dropped > 0:\n",
    "        print(f\" - Dropped {dropped} rows that became empty after cleaning.\")\n",
    "\n",
    "    # Reorder columns\n",
    "    first_cols = [\"text\", \"label\"]\n",
    "    for col in [\"source_file\", \"source_name\", \"record_id\"]:\n",
    "        if col in df.columns:\n",
    "            first_cols.append(col)\n",
    "\n",
    "    other_cols = [c for c in df.columns if c not in first_cols]\n",
    "    df = df[first_cols + other_cols]\n",
    "\n",
    "    df.to_csv(output_filename, index=False)\n",
    "    print(f\" - Saved DL cleaned: {output_filename} ({len(df)} rows)\")\n",
    "    print(\"--- [Step B1] Complete ---\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# Step B2: Deduplication\n",
    "# ===================================================================\n",
    "\n",
    "def deduplicate_dataset_dl(\n",
    "    df: pd.DataFrame,\n",
    "    output_filename: str,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Deduplicates the cleaned DL dataset.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- [Step B2] Running Deduplication ---\")\n",
    "    df = df.dropna(subset=[\"text\"])\n",
    "    before = len(df)\n",
    "\n",
    "    df = df.drop_duplicates(subset=[\"text\"], keep=\"first\")\n",
    "    after = len(df)\n",
    "\n",
    "    print(\"\\n==================================\")\n",
    "    print(\"Deduplication Report\")\n",
    "    print(f\"  Rows before: {before}\")\n",
    "    print(f\"  Rows after:  {after}\")\n",
    "    print(f\"  Removed:     {before - after}\")\n",
    "    print(\"==================================\")\n",
    "\n",
    "    df.to_csv(output_filename, index=False)\n",
    "    print(f\" - Saved DL deduplicated: {output_filename} ({after} unique rows)\")\n",
    "    print(\"--- [Step B2] Complete ---\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# Step B3: Outlier & Length Filtering\n",
    "# ===================================================================\n",
    "\n",
    "def filter_by_length_dl(\n",
    "    df: pd.DataFrame,\n",
    "    output_filename: str,\n",
    "    min_len: int,\n",
    "    max_len: int,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filters the dataset based on token count.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- [Step B3] Running Length Filtering ---\")\n",
    "    print(f\"Filtering rows with token count < {min_len} or > {max_len}...\")\n",
    "\n",
    "    # Calculate token count (simple split on space)\n",
    "    df[\"token_count\"] = df[\"text\"].str.split().str.len()\n",
    "    before = len(df)\n",
    "\n",
    "    df = df[\n",
    "        (df[\"token_count\"] >= min_len) & (df[\"token_count\"] <= max_len)\n",
    "    ]\n",
    "\n",
    "    after = len(df)\n",
    "    dropped = before - after\n",
    "\n",
    "    print(\"\\n==================================\")\n",
    "    print(\"Length Filtering Report\")\n",
    "    print(f\"  Rows before: {before}\")\n",
    "    print(f\"  Rows after:  {after}\")\n",
    "    print(f\"  Removed:     {dropped}\")\n",
    "    print(\"==================================\")\n",
    "\n",
    "    df = df.drop(columns=[\"token_count\"])\n",
    "\n",
    "    df.to_csv(output_filename, index=False)\n",
    "    print(f\" - Saved DL final: {output_filename} ({after} rows)\")\n",
    "    print(\"--- [Step B3] Complete ---\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# --- SCRIPT EXECUTION ---\n",
    "# This runs the full DL preprocessing pipeline (B1-B3).\n",
    "# ===================================================================\n",
    "\n",
    "multiprocessing.freeze_support()\n",
    "\n",
    "print(\"Starting Deep Learning (Transformer) Preprocessing Pipeline (B1-B3)...\")\n",
    "\n",
    "# Step B1: Clean\n",
    "cleaned_dl_df = clean_dataset_dl(\n",
    "    input_filename=DL_INPUT_FILE,\n",
    "    output_filename=DL_CLEANED_OUTPUT_FILE,\n",
    "    workers=multiprocessing.cpu_count()\n",
    ")\n",
    "\n",
    "if not cleaned_dl_df.empty:\n",
    "    # Step B2: Deduplicate\n",
    "    deduped_dl_df = deduplicate_dataset_dl(\n",
    "        cleaned_dl_df,\n",
    "        output_filename=DL_DEDUPED_OUTPUT_FILE\n",
    "    )\n",
    "\n",
    "    # Step B3: Filter by Length\n",
    "    final_dl_df = filter_by_length_dl(\n",
    "        deduped_dl_df,\n",
    "        output_filename=DL_FINAL_OUTPUT_FILE,\n",
    "        min_len=MIN_TOKEN_LENGTH,\n",
    "        max_len=MAX_TOKEN_LENGTH\n",
    "    )\n",
    "\n",
    "    print(\"\\nDL (Transformer) Preprocessing Pipeline (B1-B3) complete.\")\n",
    "else:\n",
    "    print(\"\\nDL Pipeline stopped: Could not load or clean input file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce68015f77ef749",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.3 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# --- Section 1: All Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm # For progress bars\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# PyTorch imports\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.optim import AdamW  # <-- CORRECTED import for AdamW\n",
    "from torch.cuda.amp import autocast, GradScaler # <-- CORRECTED import line\n",
    "\n",
    "# Transformers imports\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "\n",
    "# --- Setup ---\n",
    "warnings.filterwarnings('ignore') # Suppress warnings\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"--- Running on device: {device} ---\")\n",
    "\n",
    "# --- Section 2: Constants ---\n",
    "MODEL_NAME = 'bert-base-cased' \n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 8  # <-- START WITH A SMALL BATCH SIZE (like 8 or 4) TO AVOID MEMORY CRASHES\n",
    "N_EPOCHS = 3\n",
    "# This is the file created by your *first* preprocessing script\n",
    "DATA_FILE = \"cleaned_data/dl_dataset_final.csv\" \n",
    "\n",
    "# --- Section 3: PyTorch Dataset Class ---\n",
    "class PhishingDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# --- Section 4: Data Loading & Preprocessing ---\n",
    "# THIS IS THE SECTION THAT WAS MISSING. IT DEFINES 'le'.\n",
    "# =================================================================\n",
    "print(\"\\n--- [Section 4] Starting Data Loading & Preprocessing ---\")\n",
    "\n",
    "# 1. Load PRE-CLEANED data\n",
    "try:\n",
    "    df = pd.read_csv(DATA_FILE)\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Could not find file {DATA_FILE}\")\n",
    "    print(\"Please make sure you have run the data cleaning pipeline (the script you showed me) first.\")\n",
    "    # Stop execution if file isn't found\n",
    "    raise\n",
    "    \n",
    "print(f\"Loaded pre-cleaned dataset: {df.shape}\")\n",
    "\n",
    "# 2. Encode Labels\n",
    "# \n",
    "#  HERE IS WHERE 'le' IS CREATED\n",
    "# \n",
    "le = LabelEncoder() \n",
    "# \n",
    "# \n",
    "df['label_encoded'] = le.fit_transform(df['label'])\n",
    "print(f\"Label mapping: {list(zip(le.classes_, le.transform(le.classes_)))}\")\n",
    "NUM_CLASSES = len(le.classes_) # 'le' is now defined, so this line will work\n",
    "\n",
    "# 3. Prepare data for splitting\n",
    "X = df['text']\n",
    "y = df['label_encoded'].values\n",
    "\n",
    "# 4. Step B4: Data Splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(f\"Train set: {len(X_train)} samples | Test set: {len(X_test)} samples\")\n",
    "\n",
    "# 5. Step B5: Tokenization & Dataset Creation\n",
    "print(f\"Initializing tokenizer ({MODEL_NAME})...\")\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME) # 'tokenizer' is defined here\n",
    "\n",
    "print(\"Creating PyTorch Datasets...\")\n",
    "train_dataset = PhishingDataset(\n",
    "    texts=X_train.tolist(),\n",
    "    labels=y_train,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=MAX_LEN\n",
    ")\n",
    "test_dataset = PhishingDataset(\n",
    "    texts=X_test.tolist(),\n",
    "    labels=y_test,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=MAX_LEN\n",
    ")\n",
    "\n",
    "# 6. Step B6: Handling Class Imbalance (WeightedSampler)\n",
    "print(\"Setting up WeightedRandomSampler for training...\")\n",
    "class_counts = np.bincount(y_train)\n",
    "class_weights_per_sample = 1. / class_counts\n",
    "sample_weights = np.array([class_weights_per_sample[t] for t in y_train])\n",
    "sample_weights_tensor = torch.from_numpy(sample_weights).double()\n",
    "\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights_tensor,\n",
    "    num_samples=len(sample_weights_tensor),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "# 7. Create Final DataLoaders\n",
    "# 'train_loader' and 'test_loader' are defined here\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=sampler,\n",
    "    num_workers=2\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "# 8. (B6 Alternative) Calculate class weights for the loss function\n",
    "class_weights_loss = compute_class_weight(\n",
    "    'balanced', classes=np.unique(y_train), y=y_train\n",
    ")\n",
    "# 'class_weights_tensor' is defined here\n",
    "class_weights_tensor = torch.tensor(class_weights_loss, dtype=torch.float).to(device)\n",
    "print(f\"Weights for loss function: {class_weights_tensor.cpu().numpy()}\")\n",
    "print(\"--- [Section 4] Data Loading Complete ---\")\n",
    "# =================================================================\n",
    "\n",
    "\n",
    "# --- Section 5: Model Definition & Training Setup ---\n",
    "# This part of your script will now work because 'le', 'train_loader', etc. exist.\n",
    "# =================================================================\n",
    "print(f\"\\n--- [Section 5] Starting Model Setup ---\")\n",
    "print(f\"Loading model ({MODEL_NAME}) for {NUM_CLASSES}-class classification...\")\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=NUM_CLASSES,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")\n",
    "model.to(device) # 'model' is defined here\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "\n",
    "# Scheduler\n",
    "total_steps = len(train_loader) * N_EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Loss Function (with weights)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "# Mixed Precision Scaler\n",
    "scaler = GradScaler()\n",
    "\n",
    "# --- Section 6: Helper Functions (Training & Eval) ---\n",
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, scaler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(data_loader, desc=\"Training\", unit=\"batch\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        with autocast():\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            loss = outputs.loss \n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def eval_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\", unit=\"batch\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    return all_labels, all_preds\n",
    "\n",
    "# --- Section 7: Main Training Loop ---\n",
    "print(\"\\n--- [Section 7] Starting Training Loop ---\")\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    print(f'\\n--- Epoch {epoch + 1} / {N_EPOCHS} ---')\n",
    "    \n",
    "    avg_train_loss = train_epoch(\n",
    "        model, train_loader, loss_fn, optimizer, device, scheduler, scaler\n",
    "    )\n",
    "    print(f'Average Training Loss: {avg_train_loss:.4f}')\n",
    "    \n",
    "    print(\"Running evaluation on test set...\")\n",
    "    labels, preds = eval_model(model, test_loader, device)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    print(f'Test Accuracy: {accuracy:.4f}')\n",
    "    print(classification_report(labels, preds, target_names=le.classes_))\n",
    "\n",
    "print(\"\\n--- Training Complete ---\")\n",
    "\n",
    "# --- Section 8: Single Prediction Function ---\n",
    "def predict_single_string(text, model, tokenizer, label_encoder, device, max_len):\n",
    "    model.eval()\n",
    "    \n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        return_token_type_ids=False,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "    pred_index = torch.argmax(logits, dim=1).item()\n",
    "    predicted_label = label_encoder.inverse_transform([pred_index])[0]\n",
    "    \n",
    "    return predicted_label, pred_index\n",
    "\n",
    "# --- Section 9: Prediction Example ---\n",
    "# This was fixed: string_var is now a string, not a list.\n",
    "# =================================================================\n",
    "print(\"\\n--- [Section 9] Running Single Prediction ---\")\n",
    "string_var = \"\"\"Tuition Payment Deadline Missed ‚Äì Final Grace Period Mohit Malhotra<mohit.malhotra@mail.utoronto.ca> Dear Valued Student, This is your final warning. As of October 6, 2025, our records indicate that your Fall 2025 tuition deposit remains unpaid. You are now outside the formal payment deadline of September 30, 2025, and are at immediate risk of deregistration, loss of student status, and deactivation of all university services.Under the University of Toronto‚Äôs Financial Registration Policy, a minimum tuition deposit of $3,000‚Äì$5,000 is mandatory for all students, regardless of OSAP or external funding status. OSAP and other financial aid sources are disbursed after the term begins and do not replace the required initial deposit. To retain your course enrolment and active student status, you have been granted a final short-term grace period. Payment must now be submitted immediately using the instructions below: üîπ Interac e-Transfer Details Email: Mabintydumbuya_19@hotmail.com Memo: Include your student number onlySecurity Question: Your name\n",
    "Answer: Your student ID number‚ö†Ô∏è Do not send receipts to the transfer email address (Mabintydumbuya_19@hotmail.com)\n",
    "All receipts and communications must be submitted via this official university correspondence channel only including the security questions and answers attached to your payment.\n",
    "Failure to comply by the grace period will result in the following irreversible actions:\n",
    "Immediate removal from all enrolled courses\n",
    "Full deactivation of UTmail+, ACORN, Quercus, and all student systems\n",
    "Placement of academic and financial holds, blocking transcript access, graduation eligibility, and future enrolment\n",
    "Deregistration from the University of Toronto, with permanent implications for your academic record and immigration/visa status (if applicable)\n",
    "If you are experiencing a genuine financial emergency, you must respond today with a detailed explanation and a formal request for urgent financial assistance. Failure to respond will be treated as non-compliance.\n",
    "To resolve your status today, reply to this email with one of the following:\n",
    "A copy of your Interac e-Transfer confirmation/receipt\n",
    "A brief explanation for your non-payment and expected payment date\n",
    "A formal request for emergency financial or legal aid\n",
    "We cannot and will not hold your seat in courses without immediate compliance. This is your final opportunity to secure your enrolment and student status.\n",
    "Best regards,\n",
    "Mohit Malhotra\n",
    "Office of the Bursar\n",
    "Financial Aid & Awards University ofToronto\"\"\"\n",
    "\n",
    "# This call will now work because 'model', 'tokenizer', 'le', 'device', \n",
    "# and 'MAX_LEN' are all defined.\n",
    "label, index = predict_single_string(\n",
    "    string_var, \n",
    "    model, \n",
    "    tokenizer, \n",
    "    le, \n",
    "    device, \n",
    "    MAX_LEN\n",
    ")\n",
    "\n",
    "print(f\"Input text:    '{string_var[:100]}...'\") # Print first 100 chars\n",
    "print(f\"Predicted index: {index}\")\n",
    "print(f\"Predicted label: '{label}'\")\n",
    "\n",
    "print(\"\\n--- Script Finished ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
