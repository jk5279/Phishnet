{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Exploratory Data Analysis (EDA) for NLP Dataset",
   "id": "9c28f71a58e7322d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Part 1. ML Dataset EDA",
   "id": "36f83825086a8968"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-28T18:49:27.543060Z",
     "start_time": "2025-10-28T18:46:44.419313Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud\n",
    "import os\n",
    "\n",
    "# --- 1. Setup and Load Data ---\n",
    "print(\"--- 1. Loading Data ---\")\n",
    "out_dir = \"eda_outputs\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "sns.set(style=\"whitegrid\", context=\"notebook\")\n",
    "\n",
    "# This line is now active and will download the 'stopwords' resource.\n",
    "print(\"Checking NLTK stopwords resource...\")\n",
    "nltk.download('stopwords')\n",
    "print(\"Download complete.\")\n",
    "# ---------------------\n",
    "\n",
    "stop_words_set = set(stopwords.words('english'))\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(\"ml_dataset_final.csv\")\n",
    "    print(f\"Loaded {df.shape[0]} rows from ml_dataset_final.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: ml_dataset_final.csv not found. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- 2. Plot Target Distribution ---\n",
    "print(\"--- 2. Plotting Target Distribution ---\")\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(data=df, x='label')\n",
    "plt.title('Distribution of Labels (0=Not Phishing, 1=Phishing)')\n",
    "plt.savefig(os.path.join(out_dir, \"target_label_distribution.png\"), dpi=150)\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# --- 3. Metadata Feature Engineering ---\n",
    "print(\"--- 3. Engineering Metadata Features ---\")\n",
    "\n",
    "df['text_length'] = df['text'].apply(lambda x: len(str(x)))\n",
    "df['word_count'] = df['text'].apply(lambda x: len(str(x).split()))\n",
    "df['punct_count'] = df['text'].apply(lambda x: sum([1 for char in str(x) if char in string.punctuation]))\n",
    "df['upper_count'] = df['text'].apply(lambda x: len([word for word in str(x).split() if word.isupper()]))\n",
    "\n",
    "print(\"Metadata features created.\")\n",
    "\n",
    "\n",
    "# --- 4. Visualize Metadata Distributions ---\n",
    "print(\"--- 4. Plotting Metadata Features ---\")\n",
    "\n",
    "# Map labels for clearer plot legends\n",
    "df['label_name'] = df['label'].map({0: 'Not Phishing (0)', 1: 'Phishing (1)'})\n",
    "\n",
    "# (Optional) To see which feature is causing the error, uncomment this line:\n",
    "# print(df.groupby('label_name')[['text_length', 'word_count', 'punct_count', 'upper_count']].var())\n",
    "\n",
    "meta_features = ['text_length', 'word_count', 'punct_count', 'upper_count']\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(meta_features):\n",
    "    # --- THIS IS THE FIX ---\n",
    "    # Changed kde=True to kde=False to avoid the LinAlgError\n",
    "    sns.histplot(data=df, x=feature, hue='label_name', kde=False, ax=axes[i], bins=50, element=\"step\")\n",
    "    # ---------------------\n",
    "\n",
    "    # Truncate x-axis at 99th percentile for better plot readability\n",
    "    axes[i].set_xlim(0, df[feature].quantile(0.99))\n",
    "    axes[i].set_title(f'Distribution of {feature}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(out_dir, \"nlp_metadata_histograms.png\"), dpi=150)\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# --- 5. Correlate Engineered Features ---\n",
    "print(\"--- 5. Plotting Metadata Correlation Heatmap ---\")\n",
    "\n",
    "numeric_nlp_features = ['label', 'text_length', 'word_count', 'punct_count', 'upper_count']\n",
    "corr_matrix = df[numeric_nlp_features].corr()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=\"vlag\", center=0)\n",
    "plt.title('Correlation of Engineered Features and Label')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(out_dir, \"engineered_features_heatmap.png\"), dpi=150)\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# --- 6. Source File Analysis ---\n",
    "print(\"--- 6. Plotting Source File Analysis ---\")\n",
    "\n",
    "# Get top 10 most common source files for a clean plot\n",
    "top_sources = df['source_file'].value_counts().head(10).index\n",
    "df_top_sources = df[df['source_file'].isin(top_sources)]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.countplot(data=df_top_sources, y='source_file', hue='label', order=top_sources)\n",
    "plt.title('Label Distribution by Top 10 Source Files')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(out_dir, \"source_file_by_label.png\"), dpi=150)\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# --- 7. N-Gram Content Analysis ---\n",
    "print(\"--- 7. Analyzing N-Gram Frequencies ---\")\n",
    "\n",
    "def get_top_ngrams(corpus, n_gram_range=(1, 1), top_k=20):\n",
    "    vec = CountVectorizer(ngram_range=n_gram_range,\n",
    "                          stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:top_k]\n",
    "\n",
    "def plot_top_ngrams(ngram_list, title, ax):\n",
    "    ngrams = [item[0] for item in ngram_list]\n",
    "    frequencies = [item[1] for item in ngram_list]\n",
    "    sns.barplot(x=frequencies, y=ngrams, ax=ax, palette='viridis')\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Separate corpus by label\n",
    "spam_corpus = df[df['label'] == 1]['text'].astype(str)\n",
    "ham_corpus = df[df['label'] == 0]['text'].astype(str)\n",
    "\n",
    "# Get top n-grams\n",
    "top_spam_unigrams = get_top_ngrams(spam_corpus, n_gram_range=(1, 1), top_k=20)\n",
    "top_ham_unigrams = get_top_ngrams(ham_corpus, n_gram_range=(1, 1), top_k=20)\n",
    "top_spam_bigrams = get_top_ngrams(spam_corpus, n_gram_range=(2, 2), top_k=20)\n",
    "top_ham_bigrams = get_top_ngrams(ham_corpus, n_gram_range=(2, 2), top_k=20)\n",
    "\n",
    "# Plot n-grams\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 18))\n",
    "plot_top_ngrams(top_spam_unigrams, 'Top 20 Phishing Unigrams (Label 1)', axes[0, 0])\n",
    "plot_top_ngrams(top_ham_unigrams, 'Top 20 Not Phishing Unigrams (Label 0)', axes[0, 1])\n",
    "plot_top_ngrams(top_spam_bigrams, 'Top 20 Phishing Bigrams (Label 1)', axes[1, 0])\n",
    "plot_top_ngrams(top_ham_bigrams, 'Top 20 Not Phishing Bigrams (Label 0)', axes[1, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(out_dir, \"nlp_ngram_analysis.png\"), dpi=150)\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# --- 8. Word Clouds ---\n",
    "print(\"--- 8. Generating Word Clouds ---\")\n",
    "\n",
    "spam_text = \" \".join(text for text in spam_corpus)\n",
    "ham_text = \" \".join(text for text in ham_corpus)\n",
    "\n",
    "if spam_text and ham_text:\n",
    "    wordcloud_spam = WordCloud(stopwords=stop_words_set, background_color=\"white\", max_words=100, width=800, height=400).generate(spam_text)\n",
    "    wordcloud_ham = WordCloud(stopwords=stop_words_set, background_color=\"white\", max_words=100, width=800, height=400).generate(ham_text)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "    ax1.imshow(wordcloud_spam, interpolation='bilinear')\n",
    "    ax1.set_title('Phishing (Label 1)', fontsize=20)\n",
    "    ax1.axis(\"off\")\n",
    "\n",
    "    ax2.imshow(wordcloud_ham, interpolation='bilinear')\n",
    "    ax2.set_title('Not Phishing (Label 0)', fontsize=20)\n",
    "    ax2.axis(\"off\")\n",
    "\n",
    "    plt.savefig(os.path.join(out_dir, \"nlp_word_clouds.png\"), dpi=150)\n",
    "    plt.close()\n",
    "else:\n",
    "    print(\"Skipping word clouds (empty corpus).\")\n",
    "\n",
    "print(f\"\\n--- NLP EDA Complete ---\")\n",
    "print(f\"All outputs saved to: {out_dir}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Loading Data ---\n",
      "Checking NLTK stopwords resource...\n",
      "Download complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jk5279/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 206883 rows from ml_dataset_final.csv\n",
      "--- 2. Plotting Target Distribution ---\n",
      "--- 3. Engineering Metadata Features ---\n",
      "Metadata features created.\n",
      "--- 4. Plotting Metadata Features ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1871/2941784625.py:74: UserWarning: Attempting to set identical low and high xlims makes transformation singular; automatically expanding.\n",
      "  axes[i].set_xlim(0, df[feature].quantile(0.99))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 5. Plotting Metadata Correlation Heatmap ---\n",
      "--- 6. Plotting Source File Analysis ---\n",
      "--- 7. Analyzing N-Gram Frequencies ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1871/2941784625.py:126: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=frequencies, y=ngrams, ax=ax, palette='viridis')\n",
      "/tmp/ipykernel_1871/2941784625.py:126: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=frequencies, y=ngrams, ax=ax, palette='viridis')\n",
      "/tmp/ipykernel_1871/2941784625.py:126: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=frequencies, y=ngrams, ax=ax, palette='viridis')\n",
      "/tmp/ipykernel_1871/2941784625.py:126: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=frequencies, y=ngrams, ax=ax, palette='viridis')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 8. Generating Word Clouds ---\n",
      "\n",
      "--- NLP EDA Complete ---\n",
      "All outputs saved to: eda_outputs\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
