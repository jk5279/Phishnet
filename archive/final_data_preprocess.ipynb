{
   "cells": [
      {
         "cell_type": "markdown",
         "id": "dcc7fa79",
         "metadata": {},
         "source": [
            "## Initial Data Combination Pipeline"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 14,
         "id": "adfd0d51647a7ba9",
         "metadata": {
            "ExecuteTime": {
               "end_time": "2025-10-24T03:19:04.385572Z",
               "start_time": "2025-10-24T03:18:35.656516Z"
            }
         },
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Setting maximum CSV field size limit...\n",
                  " - CSV field size limit set to 9223372036854775807\n",
                  "Scanning for CSVs under: Dataset/raw - DO NOT OVERWRITE\n",
                  "Found 14 CSV files. Reading...\n",
                  "   - Read OK (C): Dataset/raw - DO NOT OVERWRITE/Miltchev, R. (2025)/Phishing_validation_emails.csv\n",
                  "   - Read OK (C): Dataset/raw - DO NOT OVERWRITE/Chakraborty, S. (2023)/Phishing_Email.csv\n",
                  "   - Read OK (C): Dataset/raw - DO NOT OVERWRITE/Radev, D. (2008)/fraud_email_.csv\n",
                  "   - Read OK (C): Dataset/raw - DO NOT OVERWRITE/Arifa Islam, C. (2023)/Ling.csv\n",
                  "   - Read OK (C): Dataset/raw - DO NOT OVERWRITE/Arifa Islam, C. (2023)/CEAS-08.csv\n",
                  "   - Read OK (C): Dataset/raw - DO NOT OVERWRITE/Arifa Islam, C. (2023)/Enron.csv\n",
                  "   - Read OK (C): Dataset/raw - DO NOT OVERWRITE/Arifa Islam, C. (2023)/Nazario.csv\n",
                  "   - C engine failed for Dataset/raw - DO NOT OVERWRITE/Arifa Islam, C. (2023)/TREC-05.csv: Error tokenizing data. C error: Buffer overflow caught - possible malformed input file.\n",
                  "\n",
                  "     - Retrying C + latin1 + on_bad_lines='skip'...\n",
                  "     - Fallback 1 failed: Error tokenizing data. C error: Buffer overflow caught - possible malformed input file.\n",
                  "\n",
                  "     - Retrying PYTHON engine...\n",
                  "     - Read OK (python): Dataset/raw - DO NOT OVERWRITE/Arifa Islam, C. (2023)/TREC-05.csv\n",
                  "   - Read OK (C): Dataset/raw - DO NOT OVERWRITE/Arifa Islam, C. (2023)/SpamAssasin.csv\n",
                  "   - Read OK (C): Dataset/raw - DO NOT OVERWRITE/Arifa Islam, C. (2023)/TREC-07.csv\n",
                  "   - C engine failed for Dataset/raw - DO NOT OVERWRITE/Arifa Islam, C. (2023)/TREC-06.csv: Error tokenizing data. C error: Buffer overflow caught - possible malformed input file.\n",
                  "\n",
                  "     - Retrying C + latin1 + on_bad_lines='skip'...\n",
                  "     - Fallback 1 failed: Error tokenizing data. C error: Buffer overflow caught - possible malformed input file.\n",
                  "\n",
                  "     - Retrying PYTHON engine...\n",
                  "     - Read OK (python): Dataset/raw - DO NOT OVERWRITE/Arifa Islam, C. (2023)/TREC-06.csv\n",
                  "   - Read OK (C): Dataset/raw - DO NOT OVERWRITE/Arifa Islam, C. (2023)/Nazario_5.csv\n",
                  "   - Read OK (C): Dataset/raw - DO NOT OVERWRITE/Arifa Islam, C. (2023)/Nigerian_Fraud.csv\n",
                  "   - Read OK (C): Dataset/raw - DO NOT OVERWRITE/Arifa Islam, C. (2023)/Nigerian_5.csv\n",
                  "Combining dataframes...\n",
                  " - Combined rows: 250665\n",
                  " - Saved: combined_emails.csv\n",
                  "Processing to master dataset...\n",
                  "[STEP 1] Consolidating TEXT columns...\n",
                  "   - Consolidating columns: ['Email Text', 'Text', 'body']\n",
                  "[STEP 2] Consolidating LABEL columns...\n",
                  "   - Consolidating columns: ['Email Type', 'Class', 'label']\n",
                  "[STEP 3] Standardizing labels...\n",
                  "   - Label mapping done.\n",
                  "[STEP 4] Creating final frame and cleaning...\n",
                  "\n",
                  "--- Processing Report ---\n",
                  "Total rows read: 250665\n",
                  "Rows with missing text: 637\n",
                  "Rows with unmapped labels: 772\n",
                  "\n",
                  "  > Unmapped label values (add to LABEL_MAP if needed):\n",
                  "\n",
                  "[STEP 5] Dropping rows with missing text or labels...\n",
                  "   - Dropped 792 rows.\n",
                  " - Saved master: master_email_dataset.csv (249873 rows)\n",
                  "Cleaning text using 16 workers...\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "Cleaning Emails: 100%|██████████| 249873/249873 [00:05<00:00, 48623.95it/s]\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  " - Dropped 287 rows that became empty after cleaning.\n",
                  " - Saved cleaned: master_email_dataset_cleaned.csv (249586 rows)\n",
                  "Deduplicating by 'text' column...\n",
                  "\n",
                  "==================================\n",
                  "Deduplication Report\n",
                  "  Rows before: 249586\n",
                  "  Rows after:  220495\n",
                  "  Removed:     29091\n",
                  "==================================\n",
                  " - Saved final: master_email_dataset_final.csv (220495 unique rows)\n",
                  "\n",
                  "Pipeline complete.\n"
               ]
            }
         ],
         "source": [
            "# python\n",
            "import os\n",
            "import glob\n",
            "import csv\n",
            "import sys\n",
            "import re\n",
            "import quopri\n",
            "import argparse\n",
            "import multiprocessing\n",
            "from typing import List, Dict, Optional\n",
            "\n",
            "import pandas as pd\n",
            "from bs4 import BeautifulSoup\n",
            "from tqdm.contrib.concurrent import process_map\n",
            "from bs4 import MarkupResemblesLocatorWarning\n",
            "import warnings\n",
            "warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
            "\n",
            "# =========================\n",
            "# Configuration (edit here)\n",
            "# =========================\n",
            "\n",
            "COLUMN_CONFIG = {\n",
            "    \"text_columns\": [\n",
            "        \"Email Text\",\n",
            "        \"Text\",\n",
            "        \"body\",\n",
            "    ],\n",
            "    \"label_columns\": [\n",
            "        \"Email Type\",\n",
            "        \"Class\",\n",
            "        \"label\",\n",
            "    ],\n",
            "}\n",
            "\n",
            "LABEL_MAP = {\n",
            "    \"Phishing Email\": 1,\n",
            "    \"1.0\": 1,\n",
            "    \"1\": 1,\n",
            "    \"spam\": 1,  # <-- ADDED\n",
            "    \"Safe Email\": 0,\n",
            "    \"0.0\": 0,\n",
            "    \"0\": 0,\n",
            "    \"ham\": 0,  # <-- ADDED\n",
            "}\n",
            "\n",
            "\n",
            "# =========================\n",
            "# Helpers\n",
            "# =========================\n",
            "\n",
            "def set_max_csv_field_size_limit() -> None:\n",
            "    \"\"\"Set the csv field size limit as high as possible.\"\"\"\n",
            "    print(\"Setting maximum CSV field size limit...\")\n",
            "    max_int = sys.maxsize\n",
            "    while True:\n",
            "        try:\n",
            "            csv.field_size_limit(max_int)\n",
            "            break\n",
            "        except OverflowError:\n",
            "            max_int = int(max_int / 10)\n",
            "    print(f\" - CSV field size limit set to {max_int}\")\n",
            "\n",
            "\n",
            "def safe_read_csv(path: str) -> Optional[pd.DataFrame]:\n",
            "    \"\"\"Read a CSV using robust fallbacks.\"\"\"\n",
            "    try:\n",
            "        df = pd.read_csv(path, engine=\"c\")\n",
            "        print(f\"   - Read OK (C): {path}\")\n",
            "        return df\n",
            "    except Exception as e:\n",
            "        print(f\"   - C engine failed for {path}: {e}\")\n",
            "\n",
            "    try:\n",
            "        print(\"     - Retrying C + latin1 + on_bad_lines='skip'...\")\n",
            "        df = pd.read_csv(path, engine=\"c\", encoding=\"latin1\", on_bad_lines=\"skip\")\n",
            "        print(f\"     - Read OK (C+latin1+skip): {path}\")\n",
            "        return df\n",
            "    except Exception as e:\n",
            "        print(f\"     - Fallback 1 failed: {e}\")\n",
            "\n",
            "    try:\n",
            "        print(\"     - Retrying PYTHON engine...\")\n",
            "        df = pd.read_csv(path, engine=\"python\")\n",
            "        print(f\"     - Read OK (python): {path}\")\n",
            "        return df\n",
            "    except Exception as e:\n",
            "        print(f\"     - Fallback 2 failed: {e}\")\n",
            "\n",
            "    try:\n",
            "        print(\"     - Retrying PYTHON + latin1...\")\n",
            "        df = pd.read_csv(path, engine=\"python\", encoding=\"latin1\")\n",
            "        print(f\"     - Read OK (python+latin1): {path}\")\n",
            "        return df\n",
            "    except Exception as e:\n",
            "        print(f\"     - All fallbacks failed for {path}: {e}\")\n",
            "        return None\n",
            "\n",
            "\n",
            "def consolidate_column(df: pd.DataFrame, candidates: List[str]) -> pd.Series:\n",
            "    \"\"\"Overlay multiple columns left-to-right to produce one series.\"\"\"\n",
            "    existing = [c for c in candidates if c in df.columns]\n",
            "    if not existing:\n",
            "        print(f\"   - WARNING: None of {candidates} found. Returning empty column.\")\n",
            "        return pd.Series(index=df.index, dtype=object)\n",
            "\n",
            "    print(f\"   - Consolidating columns: {existing}\")\n",
            "    col = df[existing[0]].copy()\n",
            "    for c in existing[1:]:\n",
            "        col = col.fillna(df[c])\n",
            "    return col\n",
            "\n",
            "\n",
            "# =========================\n",
            "# Step 1: Combine raw CSVs\n",
            "# =========================\n",
            "\n",
            "def combine_csvs_from_directory(root_directory: str, output_filename: str) -> pd.DataFrame:\n",
            "    if not os.path.isdir(root_directory):\n",
            "        raise FileNotFoundError(f\"Directory not found: {root_directory}\")\n",
            "\n",
            "    print(f\"Scanning for CSVs under: {root_directory}\")\n",
            "    pattern = os.path.join(root_directory, \"**\", \"*.csv\")\n",
            "    files = glob.glob(pattern, recursive=True)\n",
            "    if not files:\n",
            "        raise FileNotFoundError(f\"No CSV files found in: {root_directory}\")\n",
            "\n",
            "    print(f\"Found {len(files)} CSV files. Reading...\")\n",
            "    dfs: List[pd.DataFrame] = []\n",
            "\n",
            "    for f in files:\n",
            "        df = safe_read_csv(f)\n",
            "        if df is not None:\n",
            "            # --- MODIFICATION: Added source_name and record_id ---\n",
            "            df[\"source_file\"] = os.path.basename(f)\n",
            "            df[\"record_id\"] = df.index\n",
            "\n",
            "            # Get relative path of the file's directory from the root\n",
            "            relative_dir_path = os.path.relpath(os.path.dirname(f), root_directory)\n",
            "\n",
            "            if relative_dir_path == \".\":\n",
            "                # File is in the root, use root directory's name as source_name\n",
            "                df[\"source_name\"] = os.path.basename(os.path.normpath(root_directory))\n",
            "            else:\n",
            "                # File is in a subdirectory, use the first-level subdirectory name\n",
            "                df[\"source_name\"] = relative_dir_path.split(os.sep)[0]\n",
            "            # --- END MODIFICATION ---\n",
            "\n",
            "            dfs.append(df)\n",
            "\n",
            "    if not dfs:\n",
            "        raise RuntimeError(\"No data could be read from any CSVs.\")\n",
            "\n",
            "    print(\"Combining dataframes...\")\n",
            "    combined = pd.concat(dfs, ignore_index=True)  # Creates new 0...N index\n",
            "    combined.to_csv(output_filename, index=False)\n",
            "    print(f\" - Combined rows: {len(combined)}\")\n",
            "    print(f\" - Saved: {output_filename}\")\n",
            "    return combined\n",
            "\n",
            "\n",
            "# ============================================\n",
            "# Step 2: Consolidate text/labels and standardize\n",
            "# ============================================\n",
            "\n",
            "def process_to_master(\n",
            "    df: pd.DataFrame,\n",
            "    output_filename: str,\n",
            "    column_config: Dict[str, List[str]],\n",
            "    label_map: Dict[str, int],\n",
            ") -> pd.DataFrame:\n",
            "    print(f\"Processing to master dataset...\")\n",
            "    print(\"[STEP 1] Consolidating TEXT columns...\")\n",
            "    master_text = consolidate_column(df, column_config[\"text_columns\"])\n",
            "\n",
            "    print(\"[STEP 2] Consolidating LABEL columns...\")\n",
            "    master_label_src = consolidate_column(df, column_config[\"label_columns\"])\n",
            "\n",
            "    print(\"[STEP 3] Standardizing labels...\")\n",
            "    normalized_map = {str(k).lower().strip(): v for k, v in label_map.items()}\n",
            "    normalized_labels = master_label_src.astype(str).str.lower().str.strip()\n",
            "    master_label = normalized_labels.map(normalized_map)\n",
            "    print(\"   - Label mapping done.\")\n",
            "\n",
            "    print(\"[STEP 4] Creating final frame and cleaning...\")\n",
            "    final_df = pd.DataFrame({\"text\": master_text, \"label\": master_label})\n",
            "\n",
            "    # --- MODIFICATION: Propagate all provenance columns ---\n",
            "    for col in [\"source_file\", \"source_name\", \"record_id\"]:\n",
            "        if col in df.columns:\n",
            "            final_df[col] = df[col]\n",
            "    # --- END MODIFICATION ---\n",
            "\n",
            "    total = len(final_df)\n",
            "    na_text = final_df[\"text\"].isna().sum()\n",
            "    na_label = final_df[\"label\"].isna().sum()\n",
            "    print(\"\\n--- Processing Report ---\")\n",
            "    print(f\"Total rows read: {total}\")\n",
            "    print(f\"Rows with missing text: {na_text}\")\n",
            "    print(f\"Rows with unmapped labels: {na_label}\")\n",
            "\n",
            "    # Show unmapped original labels (help extend LABEL_MAP)\n",
            "    if na_label > 0:\n",
            "        unmapped_vals = master_label_src[final_df[\"label\"].isna()].dropna().unique()\n",
            "        print(\"\\n  > Unmapped label values (add to LABEL_MAP if needed):\")\n",
            "        for v in unmapped_vals[:20]:\n",
            "            print(f\"    - '{v}' (Type: {type(v)})\")\n",
            "        if len(unmapped_vals) > 20:\n",
            "            print(f\"    ... and {len(unmapped_vals) - 20} more\")\n",
            "\n",
            "    print(\"\\n[STEP 5] Dropping rows with missing text or labels...\")\n",
            "    before = len(final_df)\n",
            "    final_df = final_df.dropna(subset=[\"text\", \"label\"])\n",
            "    after = len(final_df)\n",
            "    print(f\"   - Dropped {before - after} rows.\")\n",
            "    if after == 0:\n",
            "        raise RuntimeError(\"Master dataset empty after cleaning.\")\n",
            "\n",
            "    final_df[\"label\"] = final_df[\"label\"].astype(int)\n",
            "\n",
            "    final_df.to_csv(output_filename, index=False)\n",
            "    print(f\" - Saved master: {output_filename} ({len(final_df)} rows)\")\n",
            "    return final_df\n",
            "\n",
            "\n",
            "# =========================\n",
            "# Step 3: Text cleaning\n",
            "# =========================\n",
            "\n",
            "def clean_email_text(text: object) -> str:\n",
            "    if not isinstance(text, str):\n",
            "        return \"\"\n",
            "\n",
            "    # Decode quoted-printable artifacts\n",
            "    try:\n",
            "        text_bytes = text.encode(\"latin-1\", errors=\"ignore\")\n",
            "        decoded_bytes = quopri.decodestring(text_bytes)\n",
            "        text = decoded_bytes.decode(\"utf-8\", errors=\"ignore\")\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    # Strip HTML\n",
            "    soup = BeautifulSoup(text, \"html.parser\")\n",
            "    text = soup.get_text(separator=\" \")\n",
            "\n",
            "    # Lowercase\n",
            "    text = text.lower()\n",
            "\n",
            "    # Replace URLs\n",
            "    text = re.sub(r\"(https?://\\S+|www\\.\\S+)\", \"[url]\", text)\n",
            "\n",
            "    # Replace emails\n",
            "    text = re.sub(r\"\\b[a-z0.9._%+-]+@[a-z0.9.-]+\\.[a-z]{2,}\\b\", \"[email]\", text)\n",
            "\n",
            "    # Keep alnum, space, and []\n",
            "    text = re.sub(r\"[^a-z0-9\\s\\[\\]]\", \"\", text)\n",
            "\n",
            "    # Normalize whitespace\n",
            "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
            "\n",
            "    return text\n",
            "\n",
            "\n",
            "def clean_dataset(\n",
            "    df: pd.DataFrame,\n",
            "    output_filename: str,\n",
            "    workers: int,\n",
            ") -> pd.DataFrame:\n",
            "    print(f\"Cleaning text using {workers} workers...\")\n",
            "    df = df.copy()\n",
            "    df[\"text\"] = df[\"text\"].fillna(\"\")\n",
            "    results = process_map(\n",
            "        clean_email_text,\n",
            "        df[\"text\"],\n",
            "        max_workers=workers,\n",
            "        chunksize=500,\n",
            "        desc=\"Cleaning Emails\",\n",
            "    )\n",
            "    df[\"text\"] = results\n",
            "\n",
            "    before = len(df)\n",
            "    df = df[df[\"text\"].str.strip().str.len() > 0]\n",
            "    dropped = before - len(df)\n",
            "    if dropped > 0:\n",
            "        print(f\" - Dropped {dropped} rows that became empty after cleaning.\")\n",
            "\n",
            "    # --- MODIFICATION: Reorder columns to include new provenance cols ---\n",
            "    first_cols = [\"text\", \"label\"]\n",
            "    for col in [\"source_file\", \"source_name\", \"record_id\"]:\n",
            "        if col in df.columns:\n",
            "            first_cols.append(col)\n",
            "    # --- END MODIFICATION ---\n",
            "\n",
            "    other_cols = [c for c in df.columns if c not in first_cols]\n",
            "    df = df[first_cols + other_cols]\n",
            "\n",
            "    df.to_csv(output_filename, index=False)\n",
            "    print(f\" - Saved cleaned: {output_filename} ({len(df)} rows)\")\n",
            "    return df\n",
            "\n",
            "\n",
            "# =========================\n",
            "# Step 4: Deduplicate\n",
            "# =========================\n",
            "\n",
            "def deduplicate_dataset(\n",
            "    df: pd.DataFrame,\n",
            "    output_filename: str,\n",
            ") -> pd.DataFrame:\n",
            "    print(\"Deduplicating by 'text' column...\")\n",
            "    df = df.dropna(subset=[\"text\"])\n",
            "    before = len(df)\n",
            "    # Keeps the first occurrence and its metadata (source_file, etc.)\n",
            "    df = df.drop_duplicates(subset=[\"text\"], keep=\"first\")\n",
            "    after = len(df)\n",
            "\n",
            "    print(\"\\n==================================\")\n",
            "    print(\"Deduplication Report\")\n",
            "    print(f\"  Rows before: {before}\")\n",
            "    print(f\"  Rows after:  {after}\")\n",
            "    print(f\"  Removed:     {before - after}\")\n",
            "    print(\"==================================\")\n",
            "\n",
            "    df.to_csv(output_filename, index=False)\n",
            "    print(f\" - Saved final: {output_filename} ({after} unique rows)\")\n",
            "    return df\n",
            "\n",
            "\n",
            "# ===================================================================\n",
            "# --- SCRIPT EXECUTION ---\n",
            "# This part runs the pipeline. It's no longer in a 'main' function.\n",
            "# ===================================================================\n",
            "\n",
            "# Manually define your paths and settings here\n",
            "class Args:\n",
            "    # --- YOUR PATH IS HERE ---\n",
            "    # Use 'r' prefix for Windows/WSL paths to handle backslashes\n",
            "    root = r\"Dataset/raw - DO NOT OVERWRITE\"\n",
            "\n",
            "    # --- Default output files ---\n",
            "    combined = \"combined_emails.csv\"\n",
            "    master = \"master_email_dataset.csv\"\n",
            "    cleaned = \"master_email_dataset_cleaned.csv\"\n",
            "    final = \"master_email_dataset_final.csv\"\n",
            "    workers = multiprocessing.cpu_count()\n",
            "\n",
            "args = Args()\n",
            "\n",
            "# This is needed for multiprocessing to work correctly in some environments\n",
            "multiprocessing.freeze_support()\n",
            "\n",
            "# --- Run the pipeline ---\n",
            "\n",
            "set_max_csv_field_size_limit()\n",
            "\n",
            "# Step 1: Combine\n",
            "combined_df = combine_csvs_from_directory(args.root, args.combined)\n",
            "\n",
            "# Step 2: Consolidate + map labels\n",
            "master_df = process_to_master(\n",
            "    combined_df, args.master, COLUMN_CONFIG, LABEL_MAP\n",
            ")\n",
            "\n",
            "# Step 3: Clean text\n",
            "cleaned_df = clean_dataset(master_df, args.cleaned, args.workers)\n",
            "\n",
            "# Step 4: Deduplicate\n",
            "deduplicate_dataset(cleaned_df, args.final)\n",
            "\n",
            "print(\"\\nPipeline complete.\")"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "b7e5e9d97191ab01",
         "metadata": {},
         "source": [
            "## Traditional ML Model Pipeline"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 16,
         "id": "9dfbdf505bf86998",
         "metadata": {
            "ExecuteTime": {
               "end_time": "2025-10-24T03:44:31.007743Z",
               "start_time": "2025-10-24T03:44:02.724993Z"
            }
         },
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Starting Machine Learning (ML) Preprocessing Pipeline (A1-A3)...\n",
                  "--- [Step A1] Running ML Text Cleaning ---\n",
                  "Loading: master_email_dataset.csv\n",
                  "Cleaning text using 16 workers...\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "ML Cleaning: 100%|██████████| 249873/249873 [00:03<00:00, 62544.03it/s] \n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  " - Dropped 287 rows that became empty after cleaning.\n",
                  " - Saved ML cleaned: ml_dataset_cleaned.csv (249586 rows)\n",
                  "--- [Step A1] Complete ---\n",
                  "\n",
                  "--- [Step A2] Running Deduplication ---\n",
                  "\n",
                  "==================================\n",
                  "Deduplication Report\n",
                  "  Rows before: 249586\n",
                  "  Rows after:  211795\n",
                  "  Removed:     37791\n",
                  "==================================\n",
                  " - Saved ML deduplicated: ml_dataset_deduped.csv (211795 unique rows)\n",
                  "--- [Step A2] Complete ---\n",
                  "\n",
                  "--- [Step A3] Running Length Filtering ---\n",
                  "Filtering rows with token count < 5 or > 2000...\n",
                  "\n",
                  "==================================\n",
                  "Length Filtering Report\n",
                  "  Rows before: 211795\n",
                  "  Rows after:  206883\n",
                  "  Removed:     4912\n",
                  "==================================\n",
                  " - Saved ML final: ml_dataset_final.csv (206883 rows)\n",
                  "--- [Step A3] Complete ---\n",
                  "\n",
                  "ML Preprocessing Pipeline (A1-A3) complete.\n"
               ]
            }
         ],
         "source": [
            "# python\n",
            "import os\n",
            "import re\n",
            "import quopri\n",
            "import multiprocessing\n",
            "import warnings\n",
            "from typing import List, Dict, Optional\n",
            "\n",
            "import pandas as pd\n",
            "from bs4 import BeautifulSoup, MarkupResemblesLocatorWarning\n",
            "from tqdm.contrib.concurrent import process_map\n",
            "\n",
            "# Suppress the BeautifulSoup URL warning\n",
            "warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
            "\n",
            "# =========================\n",
            "# Configuration\n",
            "# =========================\n",
            "\n",
            "# --- Step A1 Configuration ---\n",
            "# Use the master file from your *first* pipeline as the input\n",
            "ML_INPUT_FILE = \"master_email_dataset.csv\"\n",
            "ML_CLEANED_OUTPUT_FILE = \"ml_dataset_cleaned.csv\"\n",
            "\n",
            "# --- Step A2 Configuration ---\n",
            "ML_DEDUPED_OUTPUT_FILE = \"ml_dataset_deduped.csv\"\n",
            "\n",
            "# --- Step A3 Configuration ---\n",
            "MIN_TOKEN_LENGTH = 5\n",
            "MAX_TOKEN_LENGTH = 2000\n",
            "ML_FINAL_OUTPUT_FILE = \"ml_dataset_final.csv\"\n",
            "\n",
            "\n",
            "# ===================================================================\n",
            "# Step A1: Text Cleaning & Normalization (Aggressive)\n",
            "# ===================================================================\n",
            "\n",
            "def clean_email_text_ml(text: object) -> str:\n",
            "    \"\"\"\n",
            "    Aggressive cleaning function for the ML pipeline.\n",
            "    \"\"\"\n",
            "    if not isinstance(text, str):\n",
            "        return \"\"\n",
            "\n",
            "    # 1. Fix Encoding Artifacts (Quoted-Printable)\n",
            "    try:\n",
            "        text_bytes = text.encode(\"latin-1\", errors=\"ignore\")\n",
            "        decoded_bytes = quopri.decodestring(text_bytes)\n",
            "        text = decoded_bytes.decode(\"utf-8\", errors=\"ignore\")\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    # 2. Strip HTML Tags\n",
            "    soup = BeautifulSoup(text, \"html.parser\")\n",
            "    text = soup.get_text(separator=\" \")\n",
            "\n",
            "    # 3. Expand common contractions (before lowercasing)\n",
            "    text = re.sub(r\"n't\", \" not\", text)\n",
            "    text = re.sub(r\"'re\", \" are\", text)\n",
            "    text = re.sub(r\"'s\", \" is\", text)\n",
            "    text = re.sub(r\"'d\", \" would\", text)\n",
            "    text = re.sub(r\"'ll\", \" will\", text)\n",
            "    text = re.sub(r\"'ve\", \" have\", text)\n",
            "    text = re.sub(r\"'m\", \" am\", text)\n",
            "\n",
            "    # 4. Convert to lowercase\n",
            "    text = text.lower()\n",
            "\n",
            "    # 5. Replace URLs with [URL]\n",
            "    text = re.sub(r\"(https?://\\S+|www\\.\\S+)\", \"[URL]\", text)\n",
            "\n",
            "    # 6. Replace Emails with [EMAIL]\n",
            "    text = re.sub(r\"\\b[a-z0-9._%+-]+@[a-z0-9.-]+\\.[a-z]{2,}\\b\", \"[EMAIL]\", text)\n",
            "\n",
            "    # 7. Replace digits with [NUM]\n",
            "    text = re.sub(r\"\\d+\", \" [NUM] \", text)\n",
            "\n",
            "    # 8. Remove non-ASCII and punctuation (keep letters, spaces, and our tokens)\n",
            "    text = re.sub(r\"[^a-z\\s\\[\\]]\", \"\", text)\n",
            "\n",
            "    # 9. Normalize whitespace to a single space\n",
            "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
            "\n",
            "    return text\n",
            "\n",
            "\n",
            "def clean_dataset_ml(\n",
            "    input_filename: str,\n",
            "    output_filename: str,\n",
            "    workers: int,\n",
            ") -> pd.DataFrame:\n",
            "    \"\"\"\n",
            "    Loads the master dataset and applies the aggressive ML cleaning.\n",
            "    \"\"\"\n",
            "    print(f\"--- [Step A1] Running ML Text Cleaning ---\")\n",
            "    print(f\"Loading: {input_filename}\")\n",
            "    try:\n",
            "        df = pd.read_csv(input_filename)\n",
            "    except Exception as e:\n",
            "        print(f\"Error loading {input_filename}. Make sure it exists. Error: {e}\")\n",
            "        return pd.DataFrame()\n",
            "\n",
            "    df = df.dropna(subset=[\"text\"])\n",
            "    df = df.copy()\n",
            "\n",
            "    print(f\"Cleaning text using {workers} workers...\")\n",
            "    df[\"text\"] = df[\"text\"].fillna(\"\")\n",
            "    results = process_map(\n",
            "        clean_email_text_ml,\n",
            "        df[\"text\"],\n",
            "        max_workers=workers,\n",
            "        chunksize=500,\n",
            "        desc=\"ML Cleaning\",\n",
            "    )\n",
            "    df[\"text\"] = results\n",
            "\n",
            "    # Text Integrity Validation\n",
            "    before = len(df)\n",
            "    df = df[df[\"text\"].str.strip().str.len() > 0]\n",
            "    dropped = before - len(df)\n",
            "    if dropped > 0:\n",
            "        print(f\" - Dropped {dropped} rows that became empty after cleaning.\")\n",
            "\n",
            "    # Reorder columns\n",
            "    first_cols = [\"text\", \"label\"]\n",
            "    for col in [\"source_file\", \"source_name\", \"record_id\"]:\n",
            "        if col in df.columns:\n",
            "            first_cols.append(col)\n",
            "\n",
            "    other_cols = [c for c in df.columns if c not in first_cols]\n",
            "    df = df[first_cols + other_cols]\n",
            "\n",
            "    df.to_csv(output_filename, index=False)\n",
            "    print(f\" - Saved ML cleaned: {output_filename} ({len(df)} rows)\")\n",
            "    print(\"--- [Step A1] Complete ---\")\n",
            "    return df\n",
            "\n",
            "\n",
            "# ===================================================================\n",
            "# Step A2: Deduplication\n",
            "# ===================================================================\n",
            "\n",
            "def deduplicate_dataset_ml(\n",
            "    df: pd.DataFrame,\n",
            "    output_filename: str,\n",
            ") -> pd.DataFrame:\n",
            "    \"\"\"\n",
            "    Deduplicates the cleaned ML dataset.\n",
            "    \"\"\"\n",
            "    print(f\"\\n--- [Step A2] Running Deduplication ---\")\n",
            "    df = df.dropna(subset=[\"text\"])\n",
            "    before = len(df)\n",
            "\n",
            "    df = df.drop_duplicates(subset=[\"text\"], keep=\"first\")\n",
            "    after = len(df)\n",
            "\n",
            "    print(\"\\n==================================\")\n",
            "    print(\"Deduplication Report\")\n",
            "    print(f\"  Rows before: {before}\")\n",
            "    print(f\"  Rows after:  {after}\")\n",
            "    print(f\"  Removed:     {before - after}\")\n",
            "    print(\"==================================\")\n",
            "\n",
            "    df.to_csv(output_filename, index=False)\n",
            "    print(f\" - Saved ML deduplicated: {output_filename} ({after} unique rows)\")\n",
            "    print(\"--- [Step A2] Complete ---\")\n",
            "    return df\n",
            "\n",
            "\n",
            "# ===================================================================\n",
            "# Step A3: Outlier & Length Filtering\n",
            "# ===================================================================\n",
            "\n",
            "def filter_by_length_ml(\n",
            "    df: pd.DataFrame,\n",
            "    output_filename: str,\n",
            "    min_len: int,\n",
            "    max_len: int,\n",
            ") -> pd.DataFrame:\n",
            "    \"\"\"\n",
            "    Filters the dataset based on token count.\n",
            "    \"\"\"\n",
            "    print(f\"\\n--- [Step A3] Running Length Filtering ---\")\n",
            "    print(f\"Filtering rows with token count < {min_len} or > {max_len}...\")\n",
            "\n",
            "    df[\"token_count\"] = df[\"text\"].str.split().str.len()\n",
            "    before = len(df)\n",
            "\n",
            "    df = df[\n",
            "        (df[\"token_count\"] >= min_len) & (df[\"token_count\"] <= max_len)\n",
            "    ]\n",
            "\n",
            "    after = len(df)\n",
            "    dropped = before - after\n",
            "\n",
            "    print(\"\\n==================================\")\n",
            "    print(\"Length Filtering Report\")\n",
            "    print(f\"  Rows before: {before}\")\n",
            "    print(f\"  Rows after:  {after}\")\n",
            "    print(f\"  Removed:     {dropped}\")\n",
            "    print(\"==================================\")\n",
            "\n",
            "    df = df.drop(columns=[\"token_count\"])\n",
            "\n",
            "    df.to_csv(output_filename, index=False)\n",
            "    print(f\" - Saved ML final: {output_filename} ({after} rows)\")\n",
            "    print(\"--- [Step A3] Complete ---\")\n",
            "    return df\n",
            "\n",
            "\n",
            "# ===================================================================\n",
            "# --- SCRIPT EXECUTION ---\n",
            "# This runs the full ML preprocessing pipeline (A1-A3).\n",
            "# ===================================================================\n",
            "\n",
            "multiprocessing.freeze_support()\n",
            "\n",
            "print(\"Starting Machine Learning (ML) Preprocessing Pipeline (A1-A3)...\")\n",
            "\n",
            "# Step A1: Clean\n",
            "cleaned_ml_df = clean_dataset_ml(\n",
            "    input_filename=ML_INPUT_FILE,\n",
            "    output_filename=ML_CLEANED_OUTPUT_FILE,\n",
            "    workers=multiprocessing.cpu_count()\n",
            ")\n",
            "\n",
            "if not cleaned_ml_df.empty:\n",
            "    # Step A2: Deduplicate\n",
            "    deduped_ml_df = deduplicate_dataset_ml(\n",
            "        cleaned_ml_df,\n",
            "        output_filename=ML_DEDUPED_OUTPUT_FILE\n",
            "    )\n",
            "\n",
            "    # Step A3: Filter by Length\n",
            "    final_ml_df = filter_by_length_ml(\n",
            "        deduped_ml_df,\n",
            "        output_filename=ML_FINAL_OUTPUT_FILE,\n",
            "        min_len=MIN_TOKEN_LENGTH,\n",
            "        max_len=MAX_TOKEN_LENGTH\n",
            "    )\n",
            "\n",
            "    print(\"\\nML Preprocessing Pipeline (A1-A3) complete.\")\n",
            "else:\n",
            "    print(\"\\nML Pipeline stopped: Could not load or clean input file.\")"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "e230db5d",
         "metadata": {},
         "source": [
            "## DL Preprocessing pipeline"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 17,
         "id": "79dde4b6ad3208dc",
         "metadata": {
            "ExecuteTime": {
               "end_time": "2025-10-24T03:56:15.555087Z",
               "start_time": "2025-10-24T03:55:46.886635Z"
            }
         },
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Starting Deep Learning (Transformer) Preprocessing Pipeline (B1-B3)...\n",
                  "--- [Step B1] Running DL (Transformer) Text Cleaning ---\n",
                  "Loading: master_email_dataset.csv\n",
                  "Cleaning text using 16 workers...\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "DL Cleaning: 100%|██████████| 249873/249873 [00:04<00:00, 55606.35it/s]\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  " - Dropped 161 rows that became empty after cleaning.\n",
                  " - Saved DL cleaned: dl_dataset_cleaned.csv (249712 rows)\n",
                  "--- [Step B1] Complete ---\n",
                  "\n",
                  "--- [Step B2] Running Deduplication ---\n",
                  "\n",
                  "==================================\n",
                  "Deduplication Report\n",
                  "  Rows before: 249712\n",
                  "  Rows after:  221899\n",
                  "  Removed:     27813\n",
                  "==================================\n",
                  " - Saved DL deduplicated: dl_dataset_deduped.csv (221899 unique rows)\n",
                  "--- [Step B2] Complete ---\n",
                  "\n",
                  "--- [Step B3] Running Length Filtering ---\n",
                  "Filtering rows with token count < 5 or > 2000...\n",
                  "\n",
                  "==================================\n",
                  "Length Filtering Report\n",
                  "  Rows before: 221899\n",
                  "  Rows after:  216259\n",
                  "  Removed:     5640\n",
                  "==================================\n",
                  " - Saved DL final: dl_dataset_final.csv (216259 rows)\n",
                  "--- [Step B3] Complete ---\n",
                  "\n",
                  "DL (Transformer) Preprocessing Pipeline (B1-B3) complete.\n"
               ]
            }
         ],
         "source": [
            "# python\n",
            "import os\n",
            "import re\n",
            "import quopri\n",
            "import multiprocessing\n",
            "import warnings\n",
            "from typing import List, Dict, Optional\n",
            "\n",
            "import pandas as pd\n",
            "from bs4 import BeautifulSoup, MarkupResemblesLocatorWarning\n",
            "from tqdm.contrib.concurrent import process_map\n",
            "\n",
            "# Suppress the BeautifulSoup URL warning\n",
            "warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
            "\n",
            "# =========================\n",
            "# Configuration\n",
            "# =========================\n",
            "\n",
            "# --- Step B1 Configuration ---\n",
            "# Use the master file from your *first* pipeline as the input\n",
            "DL_INPUT_FILE = \"master_email_dataset.csv\"\n",
            "DL_CLEANED_OUTPUT_FILE = \"dl_dataset_cleaned.csv\"\n",
            "\n",
            "# --- Step B2 Configuration ---\n",
            "DL_DEDUPED_OUTPUT_FILE = \"dl_dataset_deduped.csv\"\n",
            "\n",
            "# --- Step B3 Configuration ---\n",
            "# Filter settings: remove texts with < 5 tokens or > 2000 tokens.\n",
            "# We use 2000 as a loose upper bound. The tokenizer will handle\n",
            "# the final truncation to 512 tokens.\n",
            "MIN_TOKEN_LENGTH = 5\n",
            "MAX_TOKEN_LENGTH = 2000\n",
            "DL_FINAL_OUTPUT_FILE = \"dl_dataset_final.csv\"\n",
            "\n",
            "\n",
            "# ===================================================================\n",
            "# Step B1: Text Cleaning (Transformer Path)\n",
            "# ===================================================================\n",
            "\n",
            "def clean_email_text_dl(text: object) -> str:\n",
            "    \"\"\"\n",
            "    Gentle cleaning function for the Transformer (DL) pipeline.\n",
            "\n",
            "    CRITICAL: Does NOT lowercase or remove punctuation.\n",
            "    \"\"\"\n",
            "    if not isinstance(text, str):\n",
            "        return \"\"\n",
            "\n",
            "    # 1. Fix Encoding Artifacts (Quoted-Printable)\n",
            "    try:\n",
            "        text_bytes = text.encode(\"latin-1\", errors=\"ignore\")\n",
            "        decoded_bytes = quopri.decodestring(text_bytes)\n",
            "        text = decoded_bytes.decode(\"utf-8\", errors=\"ignore\")\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    # 2. Strip HTML Tags\n",
            "    soup = BeautifulSoup(text, \"html.parser\")\n",
            "    text = soup.get_text(separator=\" \")\n",
            "\n",
            "    # 3. Replace URLs with [URL]\n",
            "    # We add spaces around tokens to ensure they are tokenized correctly\n",
            "    text = re.sub(r\"(https?://\\S+|www\\.\\S+)\", \" [URL] \", text)\n",
            "\n",
            "    # 4. Replace Emails with [EMAIL]\n",
            "    text = re.sub(r\"\\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\\b\", \" [EMAIL] \", text)\n",
            "\n",
            "    # 5. Normalize Whitespace (replace \\n, \\t, etc. with a single space)\n",
            "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
            "\n",
            "    # --- NO lowercasing, NO punctuation removal ---\n",
            "\n",
            "    return text\n",
            "\n",
            "\n",
            "def clean_dataset_dl(\n",
            "    input_filename: str,\n",
            "    output_filename: str,\n",
            "    workers: int,\n",
            ") -> pd.DataFrame:\n",
            "    \"\"\"\n",
            "    Loads the master dataset and applies the gentle DL cleaning.\n",
            "    \"\"\"\n",
            "    print(f\"--- [Step B1] Running DL (Transformer) Text Cleaning ---\")\n",
            "    print(f\"Loading: {input_filename}\")\n",
            "    try:\n",
            "        df = pd.read_csv(input_filename)\n",
            "    except Exception as e:\n",
            "        print(f\"Error loading {input_filename}. Make sure it exists. Error: {e}\")\n",
            "        return pd.DataFrame()\n",
            "\n",
            "    df = df.dropna(subset=[\"text\"])\n",
            "    df = df.copy()\n",
            "\n",
            "    print(f\"Cleaning text using {workers} workers...\")\n",
            "    df[\"text\"] = df[\"text\"].fillna(\"\")\n",
            "    results = process_map(\n",
            "        clean_email_text_dl,\n",
            "        df[\"text\"],\n",
            "        max_workers=workers,\n",
            "        chunksize=500,\n",
            "        desc=\"DL Cleaning\",\n",
            "    )\n",
            "    df[\"text\"] = results\n",
            "\n",
            "    # Text Integrity Validation\n",
            "    before = len(df)\n",
            "    df = df[df[\"text\"].str.strip().str.len() > 0]\n",
            "    dropped = before - len(df)\n",
            "    if dropped > 0:\n",
            "        print(f\" - Dropped {dropped} rows that became empty after cleaning.\")\n",
            "\n",
            "    # Reorder columns\n",
            "    first_cols = [\"text\", \"label\"]\n",
            "    for col in [\"source_file\", \"source_name\", \"record_id\"]:\n",
            "        if col in df.columns:\n",
            "            first_cols.append(col)\n",
            "\n",
            "    other_cols = [c for c in df.columns if c not in first_cols]\n",
            "    df = df[first_cols + other_cols]\n",
            "\n",
            "    df.to_csv(output_filename, index=False)\n",
            "    print(f\" - Saved DL cleaned: {output_filename} ({len(df)} rows)\")\n",
            "    print(\"--- [Step B1] Complete ---\")\n",
            "    return df\n",
            "\n",
            "\n",
            "# ===================================================================\n",
            "# Step B2: Deduplication\n",
            "# ===================================================================\n",
            "\n",
            "def deduplicate_dataset_dl(\n",
            "    df: pd.DataFrame,\n",
            "    output_filename: str,\n",
            ") -> pd.DataFrame:\n",
            "    \"\"\"\n",
            "    Deduplicates the cleaned DL dataset.\n",
            "    \"\"\"\n",
            "    print(f\"\\n--- [Step B2] Running Deduplication ---\")\n",
            "    df = df.dropna(subset=[\"text\"])\n",
            "    before = len(df)\n",
            "\n",
            "    df = df.drop_duplicates(subset=[\"text\"], keep=\"first\")\n",
            "    after = len(df)\n",
            "\n",
            "    print(\"\\n==================================\")\n",
            "    print(\"Deduplication Report\")\n",
            "    print(f\"  Rows before: {before}\")\n",
            "    print(f\"  Rows after:  {after}\")\n",
            "    print(f\"  Removed:     {before - after}\")\n",
            "    print(\"==================================\")\n",
            "\n",
            "    df.to_csv(output_filename, index=False)\n",
            "    print(f\" - Saved DL deduplicated: {output_filename} ({after} unique rows)\")\n",
            "    print(\"--- [Step B2] Complete ---\")\n",
            "    return df\n",
            "\n",
            "\n",
            "# ===================================================================\n",
            "# Step B3: Outlier & Length Filtering\n",
            "# ===================================================================\n",
            "\n",
            "def filter_by_length_dl(\n",
            "    df: pd.DataFrame,\n",
            "    output_filename: str,\n",
            "    min_len: int,\n",
            "    max_len: int,\n",
            ") -> pd.DataFrame:\n",
            "    \"\"\"\n",
            "    Filters the dataset based on token count.\n",
            "    \"\"\"\n",
            "    print(f\"\\n--- [Step B3] Running Length Filtering ---\")\n",
            "    print(f\"Filtering rows with token count < {min_len} or > {max_len}...\")\n",
            "\n",
            "    # Calculate token count (simple split on space)\n",
            "    df[\"token_count\"] = df[\"text\"].str.split().str.len()\n",
            "    before = len(df)\n",
            "\n",
            "    df = df[\n",
            "        (df[\"token_count\"] >= min_len) & (df[\"token_count\"] <= max_len)\n",
            "    ]\n",
            "\n",
            "    after = len(df)\n",
            "    dropped = before - after\n",
            "\n",
            "    print(\"\\n==================================\")\n",
            "    print(\"Length Filtering Report\")\n",
            "    print(f\"  Rows before: {before}\")\n",
            "    print(f\"  Rows after:  {after}\")\n",
            "    print(f\"  Removed:     {dropped}\")\n",
            "    print(\"==================================\")\n",
            "\n",
            "    df = df.drop(columns=[\"token_count\"])\n",
            "\n",
            "    df.to_csv(output_filename, index=False)\n",
            "    print(f\" - Saved DL final: {output_filename} ({after} rows)\")\n",
            "    print(\"--- [Step B3] Complete ---\")\n",
            "    return df\n",
            "\n",
            "\n",
            "# ===================================================================\n",
            "# --- SCRIPT EXECUTION ---\n",
            "# This runs the full DL preprocessing pipeline (B1-B3).\n",
            "# ===================================================================\n",
            "\n",
            "multiprocessing.freeze_support()\n",
            "\n",
            "print(\"Starting Deep Learning (Transformer) Preprocessing Pipeline (B1-B3)...\")\n",
            "\n",
            "# Step B1: Clean\n",
            "cleaned_dl_df = clean_dataset_dl(\n",
            "    input_filename=DL_INPUT_FILE,\n",
            "    output_filename=DL_CLEANED_OUTPUT_FILE,\n",
            "    workers=multiprocessing.cpu_count()\n",
            ")\n",
            "\n",
            "if not cleaned_dl_df.empty:\n",
            "    # Step B2: Deduplicate\n",
            "    deduped_dl_df = deduplicate_dataset_dl(\n",
            "        cleaned_dl_df,\n",
            "        output_filename=DL_DEDUPED_OUTPUT_FILE\n",
            "    )\n",
            "\n",
            "    # Step B3: Filter by Length\n",
            "    final_dl_df = filter_by_length_dl(\n",
            "        deduped_dl_df,\n",
            "        output_filename=DL_FINAL_OUTPUT_FILE,\n",
            "        min_len=MIN_TOKEN_LENGTH,\n",
            "        max_len=MAX_TOKEN_LENGTH\n",
            "    )\n",
            "\n",
            "    print(\"\\nDL (Transformer) Preprocessing Pipeline (B1-B3) complete.\")\n",
            "else:\n",
            "    print(\"\\nDL Pipeline stopped: Could not load or clean input file.\")"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "Python 3.12.3 64-bit",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 2
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython2",
         "version": "3.12.3"
      },
      "vscode": {
         "interpreter": {
            "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
         }
      }
   },
   "nbformat": 4,
   "nbformat_minor": 5
}